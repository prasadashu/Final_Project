{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Parameters for the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \n",
    "    # input for the initializer can as [5, 4, 2]\n",
    "    # first layer weight dimensions can be (4, 5) and bias dimensions can be (4, 1)\n",
    "    # second layer weight dimensions can be (2, 4) and bias dimensions can be (2, 1)\n",
    "    # DO UNDERSTAND THAT THE FIRST VALUE OF THE PARAMETER CRORRESPONDS TO THE NUMBER OF FEATURES\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    \n",
    "    # number of layers in a neural network\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, y, mini_batch_size = 10, seed = 0):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1] # 150\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Shuffling the 150 examples\n",
    "    permutation  = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]                    # contains 150 shuffled training observations\n",
    "    shuffled_y = y[:, permutation].reshape((1, m))    # contains corresponding 150 labels for the trainin observations\n",
    "    \n",
    "    # Creation of mini batches with proper set of elements\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_y = shuffled_y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Creation of a batch containing rest of the elements\n",
    "    # \"num_complete_minibatches * mini_batch_size\" contains all the proper number of elements\n",
    "    # \"num_complete_minibatches * mini_batch_size : m\" contains rest of the elements \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_y = shuffled_y[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "y = y.reshape(1, y.shape[0])\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 150)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Establishing the mini-batch\n",
    "mini_batches = random_mini_batches(X, y, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perm = np.random.permutation(X.shape[1])\n",
    "X_new = X[:, perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 9)\n",
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "# Checking the size of each mini-batch\n",
    "# Each mini batch can now be used seperately for gradient descent\n",
    "num_mini_batches = X.shape[1] // 9\n",
    "rest_batch = X.shape[1] - num_mini_batches * 9\n",
    "for i in range(num_mini_batches):\n",
    "    print(mini_batches[i][0].shape)\n",
    "    \n",
    "print(\"(4, \" + str(rest_batch) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.8  6.9  7.4  6.   5.3  6.8  7.7  6.1  5.   5.6  4.4  6.7  6.1  6.7\n",
      "   4.6  6.2  4.6  6.7  4.6  6.2  5.   5.7  6.7  6.6  5.8  4.8  5.2  7.6\n",
      "   5.5  6.   6.9  7.9  5.7  5.1  4.8  6.3  5.6  6.3  5.   6.   6.4  5.1\n",
      "   4.9  4.4  7.7  5.5  7.3  5.5  5.1  5.5  6.8  5.4  4.9  7.1  5.7  4.9\n",
      "   6.4  5.5  5.1  6.1  5.   4.8  7.2  6.3  6.3  6.9  5.6  7.   6.1  5.7\n",
      "   6.2  5.8  4.9  5.1  6.2  5.8  4.7  5.5  5.4  5.1  6.4  5.5  5.1  5.2\n",
      "   4.7  5.2  4.9  6.4  5.4  5.4  6.4  6.1  5.   6.3  5.6  6.7  6.7  5.   6.8\n",
      "   4.6  5.8  6.7  6.5  5.6  5.1  6.5  4.9  5.   4.4  4.8  6.4  6.5  5.   5.7\n",
      "   4.5  5.7  4.8  6.3  6.1  4.3  5.4  6.   5.9  6.4  7.2  6.3  6.3  5.2\n",
      "   6.5  6.   7.7  7.7  6.   5.   6.5  5.4  6.6  7.2  5.6  6.7  6.9  5.   5.7\n",
      "   5.8  5.9  5.8  5.9  5.7  5.1  6.3]\n",
      " [ 2.6  3.1  2.8  2.2  3.7  3.2  2.8  2.9  3.4  2.9  3.2  2.5  3.   3.   3.6\n",
      "   3.4  3.4  3.1  3.2  2.2  2.   4.4  3.3  2.9  2.7  3.   2.7  3.   2.5\n",
      "   2.9  3.1  3.8  2.5  2.5  3.1  3.4  3.   2.8  3.3  2.2  2.9  3.5  2.5\n",
      "   2.9  3.   2.4  2.9  2.6  3.5  3.5  3.   3.9  2.4  3.   2.8  3.1  2.8\n",
      "   4.2  3.7  2.8  3.4  3.   3.   2.9  2.5  3.2  2.7  3.2  2.6  2.8  2.8\n",
      "   2.8  3.1  3.4  2.9  2.7  3.2  2.4  3.4  3.8  3.2  2.3  3.3  3.5  3.2\n",
      "   3.4  3.1  2.7  3.7  3.   3.1  3.   3.5  2.3  2.5  3.3  3.1  3.2  2.8\n",
      "   3.1  2.7  3.   3.   3.   3.8  3.   3.   2.3  3.   3.4  3.2  2.8  3.5\n",
      "   3.8  2.3  2.6  3.4  3.3  2.8  3.   3.4  3.4  3.2  2.8  3.2  2.7  3.3\n",
      "   4.1  3.   2.7  3.8  2.6  3.   3.   3.2  3.9  3.   3.6  2.8  3.1  3.1\n",
      "   3.6  2.9  4.   3.   2.7  3.   3.   3.8  2.5]\n",
      " [ 4.   5.1  6.1  5.   1.5  5.9  6.7  4.7  1.6  3.6  1.3  5.8  4.6  5.   1.\n",
      "   5.4  1.4  4.4  1.4  4.5  3.5  1.5  5.7  4.6  5.1  1.4  3.9  6.6  4.   4.5\n",
      "   4.9  6.4  5.   3.   1.6  5.6  4.5  5.1  1.4  4.   4.3  1.4  4.5  1.4\n",
      "   6.1  3.8  6.3  4.4  1.4  1.3  5.5  1.7  3.3  5.9  4.5  1.5  5.6  1.4\n",
      "   1.5  4.7  1.5  1.4  5.8  5.6  5.   5.7  4.2  4.7  5.6  4.1  4.8  5.1\n",
      "   1.5  1.5  4.3  5.1  1.6  3.7  1.7  1.6  4.5  4.   1.7  1.5  1.3  1.4\n",
      "   1.5  5.3  1.5  4.5  5.5  4.9  1.6  4.4  3.9  5.7  5.6  1.2  4.8  1.5\n",
      "   3.9  5.2  5.5  4.1  1.9  5.2  1.4  3.3  1.3  1.6  5.3  4.6  1.3  1.7\n",
      "   1.3  3.5  1.9  4.7  4.   1.1  1.5  4.5  4.8  5.6  6.   4.9  6.   1.5\n",
      "   5.8  5.1  6.7  6.9  4.8  1.6  5.1  1.3  4.4  6.1  4.9  4.7  5.4  1.4\n",
      "   4.2  1.2  4.2  4.1  5.1  4.2  1.5  4.9]\n",
      " [ 1.2  2.3  1.9  1.5  0.2  2.3  2.   1.4  0.4  1.3  0.2  1.8  1.4  1.7\n",
      "   0.2  2.3  0.3  1.4  0.2  1.5  1.   0.4  2.1  1.3  1.9  0.1  1.4  2.1\n",
      "   1.3  1.5  1.5  2.   2.   1.1  0.2  2.4  1.5  1.5  0.2  1.   1.3  0.3\n",
      "   1.7  0.2  2.3  1.1  1.8  1.2  0.2  0.2  2.1  0.4  1.   2.1  1.3  0.1\n",
      "   2.1  0.2  0.4  1.2  0.2  0.3  1.6  1.8  1.9  2.3  1.3  1.4  1.4  1.3\n",
      "   1.8  2.4  0.1  0.2  1.3  1.9  0.2  1.   0.2  0.2  1.5  1.3  0.5  0.2\n",
      "   0.2  0.2  0.1  1.9  0.2  1.5  1.8  1.8  0.6  1.3  1.1  2.5  2.4  0.2\n",
      "   1.4  0.2  1.2  2.3  1.8  1.3  0.4  2.   0.2  1.   0.2  0.2  2.3  1.5\n",
      "   0.3  0.3  0.3  1.   0.2  1.6  1.3  0.1  0.4  1.6  1.8  2.2  1.8  1.8\n",
      "   2.5  0.1  2.2  1.6  2.2  2.3  1.8  0.2  2.   0.4  1.4  2.5  2.   1.5\n",
      "   2.1  0.2  1.3  0.2  1.5  1.   1.8  1.2  0.3  1.5]]\n"
     ]
    }
   ],
   "source": [
    "print(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    \n",
    "    # number of layers in a neural network\n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    \n",
    "    # So we are trying to create the formula for\n",
    "    # V[dW] = beta * V[dW] + (1 - beta) * dW\n",
    "    # V[db] = beta * V[db] + (1 - beta) * db\n",
    "    \n",
    "    # initialize the velocity with zeros\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parametes[\"b\"] + str(l+1).shape)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # Finally implementing the formula\n",
    "    for l in range(L):\n",
    "        # computing velocities\n",
    "        v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grad[\"db\" + str(l+1)]\n",
    "        \n",
    "        # update parameters\n",
    "        # optimizing weights\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters, v\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \n",
    "    # Declaring the parameters\n",
    "    L = len(parameters) # number of layers in a neural net\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initially all values is eaqual to zero\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        \n",
    "    return v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beata2 = 0.999, epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        # Bias corrected first moment estimate\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1 ** t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1 ** t)\n",
    "        \n",
    "        # Moving average of the squared gradients\n",
    "        s[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
    "        s[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
    "        \n",
    "        # Bias corrected second moment estimate\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
    "        \n",
    "        # Updating parameters\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
    "        \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bread and Butter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # Customizable depending on the type of model to be created\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    #Implement forward propagation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1) # IMPLEMENT AN ACTIVATION FUNCTION DU IDIOT\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2) # IMPLEMENT FINAL LAYER ACTIVATION FUNCTION\n",
    "    \n",
    "    cache = {\"Z1\" : Z1,\n",
    "             \"A1\" : A1,\n",
    "             \"Z2\" : Z2,\n",
    "             \"A2\" : A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, y):\n",
    "    \n",
    "    # Retrieving parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # Retriving values from cache\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Other parameters\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    # Implementing backprop\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A2.T) # + Regularization            Regularization can be done here\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1, 2)))   # second parameter of np.multiply is the derivative of the activation function\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T) # + Regularization             A1 is basically np.tanh(Z1), so it is like (1 - np.power(np.tanh(x), 2))\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis = 1, keepdims = True)        # derivative of tanh(x) = 1 - tanh^2(x)\n",
    "    \n",
    "    grads = {\"dW1\" : dW1,\n",
    "             \"db1\" : db1,\n",
    "             \"dW2\" : dW2,\n",
    "             \"db2\" : db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A, y):\n",
    "    \n",
    "    # sigmoid cost function for binary classification\n",
    "    m = y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A), y) + np.multiply((1 - y), np.log(1 - A))\n",
    "    cost = -(np.sum(logprobs) / m)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, y, layer_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \n",
    "    L = len(layer_dims)    # number of layers in the networks\n",
    "    cost = []              # appending cost each time\n",
    "    t = 0                  # counter for adam update\n",
    "    seed = 10              # initialize to a random value for mini_batch creation\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "        \n",
    "    #Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # incrementing seed value everytime will ensure randomization of \n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, y, mini_batch_size, seed)\n",
    "        \n",
    "        # selecting a minibatch\n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            # segregating data and target values in the minibatch\n",
    "            (minibatch_X, minibatch_y) = minibatch\n",
    "            \n",
    "            # forward propagation\n",
    "            a3, cache = forward_propagation(minibatch_X, parameters)\n",
    "            \n",
    "            # compute cost\n",
    "            cost = compute_cost(a3, minibatch_y)\n",
    "            \n",
    "            # backward propagation\n",
    "            grads = backward_propagation(parameters, cache, minibatch_X, minibatch_y)\n",
    "            \n",
    "            # update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parmeters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters == update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "                \n",
    "        \n",
    "        # Print the cost every 1000 epochs\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel(\"cost\")\n",
    "    plt.xlaber(\"epoch per 100\")\n",
    "    plt.show()\n",
    "                \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,5) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-0a303c1c0db8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlayer_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gd\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Predicting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-0528b52254c8>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, y, layer_dims, optimizer, learning_rate, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m# update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gd\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0mparmeters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters_with_gd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"momentum\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mparameters\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mupdate_parameters_with_momentum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-00f6895fa139>\u001b[0m in \u001b[0;36mupdate_parameters_with_gd\u001b[1;34m(parameters, grads, learning_rate)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,5) (2,2) "
     ]
    }
   ],
   "source": [
    "# Training our model\n",
    "# X.shape[0] = 4\n",
    "layer_dims = [X.shape[0], 5, 2, 1]                        # 4-W1-5-W2-2-W3-1\n",
    "parameters = model(X, y, layer_dims, optimizer = \"gd\")\n",
    "\n",
    "# Predicting\n",
    "predictions = predict(train_X, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
